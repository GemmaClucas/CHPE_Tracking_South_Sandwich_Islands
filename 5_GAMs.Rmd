---
title: "5_GAMs"
author: "Gemma Clucas"
date: "3/24/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mgcv)
library(pROC)
library(tidyverse)
library(knitr)
library(gratia)
library(viridis)
library(ggplot2)

select <- dplyr::select
```

Read in the data.
```{r}
data <- read.csv(file = "PresBackgroundLocationsWithEnvironmentalVariables.csv", stringsAsFactors = FALSE)

birds <- unique(data$Ptt)

names(data)

```

## Model selection using cross-validation

Run the GAMs varying the covariate and the number of knots from 3 to 6. This holds out the real and background data for each Ptt in turn, and then calculates the mean AUC, sensitivity, and specificty from all 20 runs.

I have commented this out to allow me to knit the document quickly. I read in the results of each model in the next chunk.

```{r, message = FALSE, warning = FALSE}
# var <- c("depth", "colonydist", "slope", "SST", "Height", "NorthVelocity", "EastVelocity", "chlorA")
# knots <- c(3,4,5,6)
# 
# for (l in var) {
#   for (i in knots) {
#     for (j in birds){
#       TRAIN <- data[data$Ptt!=j,]  # use all groups except 1 in training data
#       TEST<- data[data$Ptt==j,]  # keep the other group for testing data
#       # run gam with one variable, k is number of knots
#       # have to define the formula as a string first, then convert to formula and pass to GAM
#       form <- as.formula(paste("pres ~ s(", l, ", k = i)"))
#       GAM <- gam(form, data=TRAIN, bs="cs", family=binomial, select=TRUE, method='GCV.Cp')  
#       TEST$GAM_pred <- as.numeric(predict(GAM, type="response", newdata=TEST)) # predict into the test data frame
#       roc1 <- roc(TEST$pres, TEST$GAM_pred) # create a roc curve from the test data
#       AUCEVAL <- as.numeric(roc1[9]) # get the auc value
#       #also get specificity and sensitivity from the best point on the ROC curve
#       co1 <- pROC::coords(roc1, x = "best", best.method = "closest.topleft", ret=c("specificity","sensitivity")) 
#       spec1 <- as.numeric(co1[1])
#       sen1 <- as.numeric(co1[2])
#       eva <- as.data.frame(rbind(AUCEVAL,spec1,sen1))
#       names(eva) <- j
#       eva
#       if(exists("allevals")){
#         allevals <- cbind(allevals, eva)
#       }else {
#         allevals <- eva
#       }
#     }
#     # after running through all 20 birds
#     # calculate mean AUC, sensitiviy, and specificity for this number of knots
#     allevals$mean <- rowMeans(allevals)
#     # save as a csv
#     write.csv(allevals, file = paste("GAMs/", l, "_k", i, ".csv", sep = "" ))
#     # save just the mean, with the variable names to an object
#     assign(paste(l, i, sep = "_k"), 
#            as_tibble(cbind(group = names(allevals), t(allevals))) %>%
#              filter(group == "mean") %>% 
#              select(-group) %>% 
#              mutate(var = l, k = i) )
#     # remove allevals before starting the next run, since now the number of knots will change
#     rm(allevals)
#   }
# }
```

Load the csv files and create a dataframe with the AUC, sensitivty, and specifity for each model.

```{r, message = FALSE, warning = FALSE}
var <- c("depth", "colonydist", "slope", "SST", "Height", "NorthVelocity", "EastVelocity", "chlorA")
knots <- c(3,4,5,6)

# create an empty list to store the results from each round of the loop in
datalist = list()
# also initialise a counter that will increase by 1 each time the loop runs
count <- 0

for (l in var) {
  for (i in knots) {
    count <- count + 1
    read.csv(paste0("GAMs/", l, "_k", i, ".csv")) %>% 
      select("mean") %>%  
      unlist() %>% 
      c(., l, i) -> datalist[[count]]
  }
}

# bind all the list elements from the list into a dataframe, give proper names to values, and sort
dplyr::bind_rows(datalist) %>% 
  rename(covariate = "...4", 
         knots = "...5",
         AUC = mean1, 
         Sensitivity = mean2, 
         Specificity = mean3) %>%
  select(covariate, 
         knots, 
         AUC, 
         Sensitivity, 
         Specificity) %>% 
  arrange(desc(AUC)) %>% 
  kable()
  

```


So ```colonydist``` with 3 knots is the best predictor. Run models with ```colonydist``` plus the other covariates next (commented out to allow me to knit the doc).
```{r, warning = FALSE, message = FALSE}

# var <- c("depth", "slope", "SST", "Height", "NorthVelocity", "EastVelocity", "chlorA")
# knots <- c(3,4,5,6)
# 
# 
# for (l in var) {
#   for (i in knots) {
#     for (j in birds){
#       TRAIN <- data[data$Ptt!=j,]  # use all groups except 1 in training data
#       TEST<- data[data$Ptt==j,]  # keep the other group for testing data
#       # run gam with colonydist, knots = 3, and vary the other covariates and number of knots
#       # have to define the formula first as a string
#       form <- as.formula(paste("pres ~ s(colonydist, k = 3) + s(", l, ", k = i)"))
#       GAM <- gam(form, data=TRAIN, bs="cs", family=binomial, select=TRUE, method='GCV.Cp')  
#       TEST$GAM_pred <- as.numeric(predict(GAM, type="response", newdata=TEST)) # predict into the test data frame
#       roc2 <- roc(TEST$pres, TEST$GAM_pred) # create a roc curve from the test data
#       AUCEVAL2 <- as.numeric(roc2[9]) # get the auc value
#       co2 <- pROC::coords(roc2, x = "best", best.method = "closest.topleft", ret=c("specificity","sensitivity")) #also get specificity and sensitivity
#       spec2 <- as.numeric(co2[1])
#       sen2 <- as.numeric(co2[2])
#       eva2 <- as.data.frame(rbind(AUCEVAL2,spec2,sen2))
#       names(eva2) <- j
#       eva2
#       if(exists("allevals2")){
#         allevals2 <- cbind(allevals2, eva2)
#       }else {
#         allevals2 <- eva2
#       }
#     }
#     # calculate mean AUC, sensitiviy, and specificity
#     allevals2$mean <- rowMeans(allevals2)
#     # save as a csv
#     write.csv(allevals2, file = paste("GAMs/colonydist_k3+", l, "_k", i, ".csv", sep = "" ))
#     # save just the mean, with the variable names to an object
#     assign(paste("colonydist_k3_", l, "_k", i, sep = ""), 
#            as_tibble(cbind(group = names(allevals2), t(allevals2))) %>%
#              filter(group == "mean") %>% 
#              select(-group) %>% 
#              mutate(var = l, k = i) )
#     rm(allevals2)
#   }
# }
```


Read in csv files as before and sort. 
```{r, message = FALSE, warning = FALSE}
var <- c("depth", "slope", "SST", "Height", "NorthVelocity", "EastVelocity", "chlorA")
knots <- c(3,4,5,6)

# create an empty list to store the results from each round of the loop in
datalist2 = list()
# also initialise a counter that will increase by 1 each time the loop runs
count <- 0

for (l in var) {
  for (i in knots) {
    count <- count + 1
    read.csv(paste0("GAMs/colonydist_k3+", l, "_k", i, ".csv")) %>% 
      select("mean") %>%  
      unlist() %>% 
      c(., l, i) -> datalist2[[count]]
  }
}

# bind all the list elements into a dataframe, give proper names to values, and sort
dplyr::bind_rows(datalist2) %>% 
  rename(covariate = "...4", 
         knots = "...5",
         AUC = mean1, 
         Sensitivity = mean2, 
         Specificity = mean3) %>%
  select(covariate, 
         knots, 
         AUC, 
         Sensitivity, 
         Specificity) %>% 
  arrange(desc(AUC)) %>% 
  kable()
  
```
So colony distance with 3 knots and SST with 6 knots gives the highest AUC, but SST with 5 knots has an almost identical AUC.


Repeat again to add a third variable. 
```{r, warning = FALSE, message = FALSE}
# var <- c("depth", "slope", "Height", "NorthVelocity", "EastVelocity", "chlorA")
# knots <- c(3,4,5,6)
# 
# 
# for (l in var) {
#   for (i in knots) {
#     for (j in birds){
#       TRAIN <- data[data$Ptt!=j,]  # use all groups except 1 in training data
#       TEST<- data[data$Ptt==j,]  # keep the other group for testing data
#       # run gam with colonydist, knots = 3, and vary the other covariates and number of knots
#       # have to define the formula first as a string
#       form <- as.formula(paste("pres ~ s(colonydist, k = 3) + s(SST, k = 6) + s(", l, ", k = i)"))
#       GAM <- gam(form, data=TRAIN, bs="cs", family=binomial, select=TRUE, method='GCV.Cp')
#       TEST$GAM_pred <- as.numeric(predict(GAM, type="response", newdata=TEST)) # predict into the test data frame
#       roc3 <- roc(TEST$pres, TEST$GAM_pred) # create a roc curve from the test data
#       AUCEVAL3 <- as.numeric(roc3[9]) # get the auc value
#       co3 <- pROC::coords(roc3, x = "best", best.method = "closest.topleft", ret=c("specificity","sensitivity")) #also get specificity and sensitivity
#       spec3 <- as.numeric(co3[1])
#       sen3 <- as.numeric(co3[2])
#       eva3 <- as.data.frame(rbind(AUCEVAL3,spec3,sen3))
#       names(eva3) <- j
#       eva3
#       if(exists("allevals3")){
#         allevals3 <- cbind(allevals3, eva3)
#       }else {
#         allevals3 <- eva3
#       }
#     }
#     # calculate mean AUC, sensitiviy, and specificity
#     allevals3$mean <- rowMeans(allevals3)
#     # save as a csv
#     write.csv(allevals3, file = paste("GAMs/colonydist_k3+SST_k6+", l, "_k", i, ".csv", sep = "" ))
#     # save just the mean, with the variable names to an object
#     assign(paste("colonydist_k3_SST_k6_", l, "_k", i, sep = ""),
#            as_tibble(cbind(group = names(allevals3), t(allevals3))) %>%
#              filter(group == "mean") %>%
#              select(-group) %>%
#              mutate(var = l, k = i) )
#     rm(allevals3)
#   }
# }
```


Combine outputs from models with 3 covariates.
```{r, message = FALSE, warning = FALSE}
var <- c("depth", "slope", "Height", "NorthVelocity", "EastVelocity", "chlorA")
knots <- c(3,4,5,6)

# create an empty list to store the results from each round of the loop in
datalist3 = list()
# also initialise a counter that will increase by 1 each time the loop runs
count <- 0

for (l in var) {
  for (i in knots) {
    count <- count + 1
    read.csv(paste0("GAMs/colonydist_k3+SST_k6+", l, "_k", i, ".csv")) %>% 
      select("mean") %>%  
      unlist() %>% 
      c(., l, i) -> datalist3[[count]]
  }
}

# bind all the list elements into a dataframe, give proper names to values, and sort
dplyr::bind_rows(datalist3) %>% 
  rename(covariate = "...4", 
         knots = "...5",
         AUC = mean1, 
         Sensitivity = mean2, 
         Specificity = mean3) %>%
  select(covariate, 
         knots, 
         AUC, 
         Sensitivity, 
         Specificity) %>% 
  arrange(desc(AUC)) %>% 
  kable()
  
```

The increase in the AUC is only very marginal when adding a third covariate -> stick to just two covariates (distance to colony and SST) and run with all the data next.

### Run the model with all of the data

```{r}
GAM <- gam(pres ~ s(colonydist, k = 3) + s(SST, k = 6), data=data, bs="cs", family=binomial, select=TRUE, method='GCV.Cp')
summary(GAM)
#gratia::draw(GAM, residuals = TRUE)
```

```{r}
gam.check(GAM, rep = 300)
```

The residuals look pretty good in the plots on the left. The plots on the right are always going to look weird because it's binomial I think.

### Does select = TRUE or cubic spline with shrinkage effect results?

```{r}
GAM <- gam(pres ~ s(colonydist, k = 3) + s(SST, k = 6), data=data, family=binomial, select=TRUE, method='GCV.Cp')
summary(GAM)
#gratia::draw(GAM, residuals = TRUE)
```
Removing the cubic spline with shrinkage ```(bs = 'cs')``` does not seem to alter the result.

```{r}
GAM <- gam(pres ~ s(colonydist, k = 3) + s(SST, k = 6), data=data, family=binomial, method='GCV.Cp')
summary(GAM)
#gratia::draw(GAM, residuals = TRUE)
```
Removing the double penalty ```select = TRUE``` does seem to allow the number of basis functions for colonydist and SST to increase very slightly within the bounds I have set, therefore keep it in going forwards.  

### What about a different way to do model selection?

Here I was playing around with the model selection method, to see whether GVC.Cp or REML does a better job. To use these methods, you have to run the full model and then let it determine which covariates are important and how many wiggles they should have. I have switched to the bam method as it's quicker to compute for large datasets.
```{r}
BAM <- bam(pres ~ s(colonydist) + s(SST) +s(Height) + s(slope) + s(NorthVelocity) + s(EastVelocity) + s(chlorA) + s(depth), data=data, family=binomial, select=TRUE, method='GCV.Cp')
summary(BAM)
gratia::draw(BAM, residuals = TRUE)
```

```{r}
BAM <- bam(pres ~ s(colonydist) + s(SST) +s(Height) + s(slope) + s(NorthVelocity) + s(EastVelocity) + s(chlorA) + s(depth), data=data, family=binomial, select=TRUE, method="fREML")
summary(BAM)
gratia::draw(BAM, residuals = TRUE)
```


It seems like both methods end up overfitting the data, as they both conclude that all covariates are important with lots of wiggles, which is just biologically very unrealistic. This is probably due to autocorrelation in the data, so the forward model selection with cross-validation is the better way to do things.


## Make predictions from the model

Re-run the final model.
```{r}
GAM <- gam(pres ~ s(colonydist, k = 3) + s(SST, k = 6), data=data, bs="cs", family=binomial, select=TRUE, method='GCV.Cp')

```

Load packages.
```{r}
library(raster)
library(rgdal)
library(ncdf4)
library(viridis)
```

### 1. Predict around Saunders to check that the distribution looks ok

This just creates a raster across the study area using the bathymetry data.
```{r}
# Read in bathymetry raster and crop to extent of the study area
SSI_bath_WGS84 <- raster("ssi_geotif/full_ssi18a.tif") %>% 
  projectRaster(., crs=crs("+init=epsg:4326")) %>% 
  crop(., c(-27.96598, -24.623, -58.41806, -57.28708))

x <- SSI_bath_WGS84

# Project to LAEA to find extent in metres
y <- projectRaster(x, crs = CRS("+proj=laea +lon_0=-26 +lat_0=-58 +units=m"))
extent(y)
```


Crop out land from the bathymetry raster using ```mask()``` 
```{r}
# Read in shapefile for land
SSI_WGS84 <- readOGR("Seamask.shp") %>% 
  crop(., c(450000, 1095192, -795043.9, -100000)) %>% 
  spTransform(., crs("+init=epsg:4326"))

# # Convert to LAEA
# x <- projectRaster(x, crs = CRS("+proj=laea +lon_0=-26 +lat_0=-58 +units=m"))
# # Downsample
# x <- aggregate(x, fact = 5)
# # Convert back to WGS84
# x <- projectRaster(x, crs = crs("+init=epsg:4326"))

# I have commented out this section as I don't actually need to downsample this layer, I'll just sample it with my 1km x 1km grid

# Cut out land
mask <- mask(x, SSI_WGS84, inverse=F)

# Plot to check
plot(mask, col=viridis(100))
plot(mask, xlim = c(-26.6, -26.2), ylim = c(-57.9, -57.7), col=viridis(100))
```




Make a raster for the distance from colony.
```{r}
colony_lat<- -57.808 
colony_lon<- -26.404

# Find the colony cell in the study area raster
j <- cellFromXY(mask, cbind(colony_lon, colony_lat))
# Change the value of the cell where the colony is to 2 (all the other cells are 1)
mask[j]<-2 


# Create a distance raster from the colony
# Moving through land is prevented by omiting cells with NA values
dist <- gridDistance(mask, origin=2, omit=NA)
plot(dist, col=viridis(100))
plot(dist, xlim = c(-26.6, -26.2), ylim = c(-57.9, -57.7), col=viridis(100))
```


Get the sea surface temperature data that I downloaded previously.
```{r}
# Open data - note there is a dataset for Jan and one for Feb
nc_data_Jan <- nc_open('global-analysis-forecast-phy-001-024-monthly_January2020.nc')
nc_data_Feb <- nc_open('global-analysis-forecast-phy-001-024-monthly_February2020.nc')
# Extract longs and lats
nc_lon <- ncvar_get(nc_data_Jan, "longitude")
nc_lat <- ncvar_get(nc_data_Jan, "latitude", verbose = F)
# Create an array of the SST data
temp.array.Jan <- ncvar_get(nc_data_Jan, "thetao")
temp.array.Feb <- ncvar_get(nc_data_Feb, "thetao")
# Take just the surface layer
surface.temp.Jan <- temp.array.Jan[, , 1] 
surface.temp.Feb <- temp.array.Feb[, , 1] 
# Make it into a raster
SST_Jan <- raster(t(surface.temp.Jan), 
                   xmn=min(nc_lon), 
                   xmx=max(nc_lon), 
                   ymn=min(nc_lat), 
                   ymx=max(nc_lat), 
                   crs=crs("+init=epsg:4326")) %>% 
  flip(., direction = "y")
# Same for Feb
SST_Feb <- raster(t(surface.temp.Feb), 
                   xmn=min(nc_lon), 
                   xmx=max(nc_lon), 
                   ymn=min(nc_lat), 
                   ymx=max(nc_lat), 
                   crs=crs("+init=epsg:4326")) %>% 
  flip(., direction = "y")
# Calculate weighted mean (weighting by the number of observations in January vs February)
wMeanSST <- stack(c(SST_Jan, SST_Feb)) %>% 
  weighted.mean(., w = c(0.64, 0.36))
# Plot for study area
raster::plot(wMeanSST, xlim = c(-27.96598, -24.623), ylim = c(-58.41806, -57.28708), col=viridis(100))
raster::plot(wMeanSST, xlim = c(-26.6, -26.2), ylim = c(-57.9, -57.7), col=viridis(100))

```


Make a new raster with a 1km x 1km resolution. This is what we will use to sample the distance and SST rasters. The values for the max and min long/lat are from the depth raster to LAEA after cropping it to the study area extent (above).
```{r}
long <- seq(-119511.7 , 83876.27, 1000) #first minimum longitude in m, then max, 1000 is 1km
# head(long)
lat <- seq(-49280.75, 80319.25, 1000)
# head(lat)
Saunders_1KmPoints <- expand.grid(long,lat)
names(Saunders_1KmPoints) <- c("Lon","Lat")
# make it spatial
coordinates(Saunders_1KmPoints) <- ~Lon+Lat
proj4string(Saunders_1KmPoints) <- CRS("+proj=laea +lon_0=-26 +lat_0=-58 +units=m")


# change it back to wgs84 to extract the environmental data (those layers are WGS84)
Saunders_1KmPoints <- spTransform(Saunders_1KmPoints, CRS = proj4string(dist))
#plot(Saunders_1KmPoints)

Saunders_1KmPoints$SST <- raster::extract(wMeanSST, Saunders_1KmPoints)
Saunders_1KmPoints$colonydist <- raster::extract(dist, Saunders_1KmPoints)

view(as.data.frame(Saunders_1KmPoints))
```

Make predictions.
```{r}
Saunders_1KmPoints$GAM_pred <- as.numeric(predict(GAM, type="response", newdata = Saunders_1KmPoints))
```

Make a new raster with the correct resolution and extent of the 1km grid, then use it to convert the grid into a raster for plotting.
```{r}
# new raster in LAEA
r <- raster(ncols = 204, nrows = 130, crs = CRS("+proj=laea +lon_0=-26 +lat_0=-58 +units=m")) 
# define the extent of the raster
extent(r) <- c(-119511.7 , 83876.27, -49280.75, 80319.25  )
# change points into LAEA as well
Saunders_1KmPoints <- spTransform(Saunders_1KmPoints, CRS = "+proj=laea +lon_0=-26 +lat_0=-58 +units=m")
# rasterize the predicted values
r3 <- rasterize(Saunders_1KmPoints, r, 'GAM_pred', fun=mean)
# project back to WGS84 for plotting
r3 <- projectRaster(from = r3, to = dist)
plot(r3, col=viridis(100))

```

This was the easiest way to plot the island with no background.
```{r}
bathy_mask <- mask(SSI_bath_WGS84, SSI_WGS84, inverse=F)
dat <- marmap::as.bathy(bathy_mask)

All <- read.csv("Chick-rearing_trips/All_chick-rearing_trips.csv", stringsAsFactors = FALSE)

autoplot(dat, geom=c("raster", "contour"), coast = FALSE, colour="white", size=0.1) + 
  scale_fill_gradient(low = "white", high = "white") +
  ylab("Latitude") +
  xlab("Longitude") +
  labs(fill = "Depth") +
  stat_density2d(data = as.data.frame(All),
                 aes(x = LON, y = LAT, alpha = ..level..), 
                 geom = "polygon", 
                 fill = "red") +
  scale_alpha(range=c(0.2,0.9),guide=FALSE)
```

```{r}
ggplot(data = as.data.frame(All), aes(x = LON, y = LAT)) +
  geom_density_2d_filled()

# make all raster values the same
mask2 <- mask > -Inf
# turn into polygon
pp <- rasterToPolygons(mask2, dissolve=TRUE)
plot(pp)

ggplot(data = pp, aes(x=long, y = lat)) +
  geom_polygon(fill = "white")

```

